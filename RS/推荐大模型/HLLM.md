# HLLM:Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling
[原文链接]([2409.12740](https://arxiv.org/pdf/2409.12740))
## 0 摘要：


## 背景


## 1 论文解决的问题：
尽管这些技术有所进步，但将语言模型与推荐系统相结合在复杂性和有效性方面仍面临显著挑战。
* 其中一个问题是，将用户的使用历史行为序列以文本形式输入到语言模型中会导致输入序列非常长。因此，**LLM需要比基于 ID-emb的方法更长的序列来表示相同时间段内的用户行为**，而语言模型中的自注意力模块的复杂性会随着序列长度的增加而呈平方级增长。
* 此外，**推荐单个项目需要生成多个文本标记，这会导致多次前向传递**，从而降低效率。在有效性方面，现有的基于语言模型的方法相较于传统方法在性能上的提升并不显著，这引发了关于语言模型潜力是否已得到充分释放的疑问。

## 2 论文创新点：
为解决这些挑战，本文提出了“分层大型语言模型（HLLM）”架构。该方法首先使用大型语言模型（LLM）来提取项目特征。为了使 LLM 能够有效地提取这些特征，会在每个项目的详细文本描述末尾添加一个特殊的标记。然后将这个增强后的描述输入到 LLM 中（称为“项目 LLM”），对应于该特殊标记的输出被用作项目特征。这些项目特征随后被输入到第二个 LLM（称为“用户 LLM”）中，以模拟用户兴趣并预测未来行为。通过将大量的项目描述转换为简洁的嵌入，行为序列的长度被缩短到基于 ID 的模型的长度，与其他基于文本的 LLM 推荐模型相比，显著降低了计算复杂度。我们还验证了 HLLM 比基于 ID 的模型具有显著的训练效率优势，因为它仅用少量的训练数据就能超越基于 ID 的模型。


### 2.1 预训练数据集的构建：


### 2.1 技巧：


## 4 模型结构与实现代码：


## 5 实验与分析：

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0NTYwNzk4MjIsNDg3NjkxMTE2XX0=
-->