
# HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou
[原文链接]()
## 0 摘要：
在本文中，我们介绍了快手短视频服务中遇到的实际问题以及从中吸取的经验教训。在行业中，广泛使用的多任务框架是专家混合（MoE）范式，它总是为每个任务引入一些共享和特定的专家，然后使用门网络来衡量相关专家的贡献。尽管 MoE 取得了显著的改进，但在我们的迭代过程中，我们仍然观察到三个严重影响模型性能的异常情况：（1）专家崩溃：我们发现专家的输出分布差异显著，一些专家在使用 ReLU 时超过 90% 的激活值为零，这使得门网络难以分配公平的权重来平衡专家。（2）专家退化：理想情况下，共享专家旨在同时为所有任务提供预测信息。然而，我们发现一些共享专家仅被一个任务占用，这表明共享专家失去了其能力，退化成了特定专家。（3）专家欠拟合：在我们的服务中，有数十种行为任务需要预测，但我们发现一些数据稀疏的预测任务往往会忽略其特定专家，而赋予共享专家较大的权重。原因可能是共享专家能够从密集任务中感知到更多的梯度更新和知识，而特定专家由于其行为稀疏，容易陷入欠拟合。

## 背景


## 1 论文解决的问题：


## 2 论文创新点：


### 2.1 预训练数据集的构建：


### 2.1 技巧：


## 4 模型结构与实现代码：


## 5 实验与分析：

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTk0NTg5Mzc4MV19
-->