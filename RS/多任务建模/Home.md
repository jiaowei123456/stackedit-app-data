
# HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou
[原文链接]()
## 0 摘要：
在本文中，我们介绍了快手短视频服务中遇到的实际问题以及从中吸取的经验教训。在行业中，广泛使用的多任务框架是专家混合（MoE）范式，它总是为每个任务引入一些共享和特定的专家，然后使用门网络来衡量相关专家的贡献。尽管 MoE 取得了显著的改进，但在我们的迭代过程中，我们仍然观察到三个严重影响模型性能的异常情况：**（1）专家崩溃：我们发现专家的输出分布差异显著，一些专家在使用 ReLU 时超过 90% 的激活值为零，这使得门网络难以分配公平的权重来平衡专家。（2）专家退化：理想情况下，共享专家旨在同时为所有任务提供预测信息。然而，我们发现一些共享专家仅被一个任务占用，这表明共享专家失去了其能力，退化成了特定专家。（3）专家欠拟合：在我们的服务中，有数十种行为任务需要预测，但我们发现一些数据稀疏的预测任务往往会忽略其特定专家，而赋予共享专家较大的权重**。原因可能是共享专家能够从密集任务中感知到更多的梯度更新和知识，而特定专家由于其行为稀疏，容易陷入欠拟合。
		受这些观察结果的启发，我们提出了 HoME，旨在实现一个简单、高效且平衡的多任务学习的 MoE 系统。具体而言，我们进行了三项富有洞察力的改进：（1）专家归一化和 Swish 机制，以对齐专家输出分布并避免专家崩溃。（2）层级掩码机制，以增强任务间的共享效率，减少占用问题并避免专家性能下降。（3）特征门控和自门控机制，确保每个专家都能获得适当的梯度，以最大化其有效性。据我们所知，这是首个专注于提升多任务 MoE 系统稳定性的研究工作，并且我们进行了广泛的离线和在线实验（离线平均提升 0.52% GAUC，用户在线每小时提升 0.954%）。通过实验和消融分析来证明我们的 HoME 技术的有效性。HoME 已在快手的短视频服务中部署，每天为 4 亿用户提供服务。

## 背景


## 1 论文解决的问题：
1）专家崩溃：我们发现专家的输出分布差异显著，一些专家在使用 ReLU 时超过 90% 的激活值为零，这使得门网络难以分配公平的权重来平衡专家。（2）专家退化：理想情况下，共享专家旨在同时为所有任务提供预测信息。然而，我们发现一些共享专家仅被一个任务占用，这表明共享专家失去了其能力，退化成了特定专家。（3）专家欠拟合：在我们的服务中，有数十种行为任务需要预测，但我们发现一些数据稀疏的预测任务往往会忽略其特定专家，而赋予共享专家较大的权重。

## 2 论文创新点：
* 专家归一化与 Swish 机制：为了平衡专家输出的差异并避免专家权重的坍缩，我们首先为每个专家引入了[1, 16]的标准化操作，将他们的输出投影到接近正态分布的范围，即专家输出分布 ≈ N (0， I)。然而，在这种设置下，我们发现直接进行标准化也会导致在 ReLU 函数之后存在过多的 0。原因可能是标准化后的专家输出的均值接近 0，因此一半的输出会小于 0，然后在 ReLU 下被激活为 0。为了缓解零导数梯度现象，我们使用 Swish [28] 函数来替代 ReLU 函数，以提高参数的利用率并加快训练过程。自从引入标准化和 Swish 设置以来，所有专家的输出都能够达到相似的数值大小，这有助于我们的门控网络分配相当的权重。
* 层级掩码机制：为解决专家占用问题以及避免专家能力下降（也称为任务冲突跷跷板问题），在本文中，我们提出了一种简单而有效的级联层级掩码机制来缓解这种冲突。具体而言，我们插入一个前序元专家网络，将不同的任务进行分组，以扩展标准化的多专家系统。如图 1 所示，我们的短视频行为任务可以根据其先前的相关性手动分为两个元类别：（1）被动观看时间任务，例如“长视屏”；（2）主动交互任务，例如“评论”。因此，我们可以预先对粗粒度的元类别专家进行建模，然后以以下方式支持每个任务：每个任务不仅应该有完全共享的全局专家，还应该有部分共享的同类专家。
* 特征门和自我门机制：为了增强我们稀疏任务专家的训练效果，我们提出了两种门机制，以确保它们能够获得适当的梯度以最大化其有效性：特征门机制和自我门机制。考虑到同一层的专家们总是使用相同的输入特征，但不同的专家会得到不同的梯度。因此，相同的特征输入可能会带来梯度方面潜在的风险在多个专家参数优化方面存在冲突。为此，我们首先提出了特征门机制，以保护稀疏任务专家训练，同时对灵活的专家输入进行私有化处理。此外，最新的多专家门模型（MMoE）研究表明，更深的专家网络堆叠结构能够带来更强大的预测能力。然而，在我们的实验中，我们发现原始门网络会逐层稀释梯度，这不利于稀疏任务专家的训练。为了确保顶层的梯度能够有效地传递到底层，并稳定更深的 MMoE 系统训练，我们进一步设计了自门机制，以将相邻相关的专家以残差方式连接起来。

### 2.1 预训练数据集的构建：


### 2.1 技巧：


## 4 模型结构与实现代码：


## 5 实验与分析：

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTYzMzAxMzc3NCwyMzEwNDkzNjAsNTQwOD
IyNzg2LC02MDUwMzM1OTgsLTk0NTg5Mzc4MV19
-->