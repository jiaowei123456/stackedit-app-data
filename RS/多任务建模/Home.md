
# HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou
[原文链接]()
## 0 摘要：
在本文中，我们介绍了快手短视频服务中遇到的实际问题以及从中吸取的经验教训。在行业中，广泛使用的多任务框架是专家混合（MoE）范式，它总是为每个任务引入一些共享和特定的专家，然后使用门网络来衡量相关专家的贡献。尽管 MoE 取得了显著的改进，但在我们的迭代过程中，我们仍然观察到三个严重影响模型性能的异常情况：**（1）专家崩溃：我们发现专家的输出分布差异显著，一些专家在使用 ReLU 时超过 90% 的激活值为零，这使得门网络难以分配公平的权重来平衡专家。（2）专家退化：理想情况下，共享专家旨在同时为所有任务提供预测信息。然而，我们发现一些共享专家仅被一个任务占用，这表明共享专家失去了其能力，退化成了特定专家。（3）专家欠拟合：在我们的服务中，有数十种行为任务需要预测，但我们发现一些数据稀疏的预测任务往往会忽略其特定专家，而赋予共享专家较大的权重**。原因可能是共享专家能够从密集任务中感知到更多的梯度更新和知识，而特定专家由于其行为稀疏，容易陷入欠拟合。
		受这些观察结果的启发，我们提出了 HoME，旨在实现一个简单、高效且平衡的多任务学习的 MoE 系统。具体而言，我们进行了三项富有洞察力的改进：（1）专家归一化和 Swish 机制，以对齐专家输出分布并避免专家崩溃。（2）层级掩码机制，以增强任务间的共享效率，减少占用问题并避免专家性能下降。（3）特征门控和自门控机制，确保每个专家都能获得适当的梯度，以最大化其有效性。据我们所知，这是首个专注于提升多任务 MoE 系统稳定性的研究工作，并且我们进行了广泛的离线和在线实验（离线平均提升 0.52% GAUC，用户在线每小时提升 0.954%）。通过实验和消融分析来证明我们的 HoME 技术的有效性。HoME 已在快手的短视频服务中部署，每天为 4 亿用户提供服务。

## 背景


## 1 论文解决的问题：
1）专家崩溃：我们发现专家的输出分布差异显著，一些专家在使用 ReLU 时超过 90% 的激活值为零，这使得门网络难以分配公平的权重来平衡专家。（2）专家退化：理想情况下，共享专家旨在同时为所有任务提供预测信息。然而，我们发现一些共享专家仅被一个任务占用，这表明共享专家失去了其能力，退化成了特定专家。（3）专家欠拟合：在我们的服务中，有数十种行为任务需要预测，但我们发现一些数据稀疏的预测任务往往会忽略其特定专家，而赋予共享专家较大的权重。

## 2 论文创新点：


### 2.1 预训练数据集的构建：


### 2.1 技巧：


## 4 模型结构与实现代码：


## 5 实验与分析：

<!--stackedit_data:
eyJoaXN0b3J5IjpbMjMxMDQ5MzYwLDU0MDgyMjc4NiwtNjA1MD
MzNTk4LC05NDU4OTM3ODFdfQ==
-->